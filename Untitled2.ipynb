{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqzjl4EJufsd7HeTFwudTI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bekjarska221153/Dizajn_i_arhitektura_na_softver_domasna/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM1LwNHs29PZ",
        "outputId": "bcaad36d-10ad-4718-be31-1a50c6f5e2d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.26.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading selenium-4.26.1-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: sortedcontainers, wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.26.1 sortedcontainers-2.4.0 trio-0.27.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install selenium\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install webdriver_manager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKT6PUAl3UAN",
        "outputId": "0f47e686-60b4-4fcc-ee18-7af4b6d46ebc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting webdriver_manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver_manager)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (24.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2024.8.30)\n",
            "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv, webdriver_manager\n",
            "Successfully installed python-dotenv-1.0.1 webdriver_manager-4.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta"
      ],
      "metadata": {
        "id": "wuE4fi2n3IMF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_kmb_data():\n",
        "    # Define the issuer code and date range\n",
        "    issuer_code = 'KMB'\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=365 * 10)  # Start date 10 years ago\n",
        "\n",
        "    # Base URL and headers\n",
        "    base_url = \"https://www.mse.mk/mk/stats/symbolhistory/kmb\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    # Initialize a list to store collected data\n",
        "    collected_data = []\n",
        "\n",
        "    while start_date < end_date:\n",
        "        # Set the range to a maximum of 1 year\n",
        "        range_end_date = min(start_date + timedelta(days=365), end_date)\n",
        "\n",
        "        # Convert dates to the required format for the URL parameters\n",
        "        start_date_str = start_date.strftime('%d.%m.%Y')\n",
        "        range_end_date_str = range_end_date.strftime('%d.%m.%Y')\n",
        "\n",
        "        # Prepare payload for the post request\n",
        "        payload = {\n",
        "            'Code': issuer_code,\n",
        "            'FromDate': start_date_str,\n",
        "            'ToDate': range_end_date_str,\n",
        "            'action': 'Прикажи'  # Submit button's action text in Macedonian\n",
        "        }\n",
        "\n",
        "        # Send POST request to retrieve the data\n",
        "        response = requests.post(base_url, headers=headers, data=payload)\n",
        "        response.raise_for_status()  # Check for request errors\n",
        "\n",
        "        # Parse the response HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Locate the results table and rows within it\n",
        "        table = soup.find(\"table\", {\"id\": \"resultsTable\"})\n",
        "        if not table:\n",
        "            print(f\"No data available for the period {start_date_str} to {range_end_date_str}\")\n",
        "            start_date = range_end_date + timedelta(days=1)\n",
        "            continue\n",
        "\n",
        "        # Process each row in the table (skipping the header row)\n",
        "        rows = table.find_all(\"tr\")[1:]\n",
        "        for row in rows:\n",
        "            columns = row.find_all(\"td\")\n",
        "            if len(columns) >= 9:\n",
        "                data = {\n",
        "                    'Issuer': issuer_code,\n",
        "                    'Date': columns[0].text.strip(),\n",
        "                    'Last Transaction Price': columns[1].text.strip(),\n",
        "                    'Max': columns[2].text.strip(),\n",
        "                    'Min': columns[3].text.strip(),\n",
        "                    'Average Price': columns[4].text.strip(),\n",
        "                    '% Change': columns[5].text.strip(),\n",
        "                    'Quantity': columns[6].text.strip(),\n",
        "                    'Trading Volume (Denars)': columns[7].text.strip(),\n",
        "                    'Total Volume (Denars)': columns[8].text.strip()\n",
        "                }\n",
        "                collected_data.append(data)\n",
        "\n",
        "        # Move to the next one-year range\n",
        "        start_date = range_end_date + timedelta(days=1)\n",
        "\n",
        "    # Convert collected data to a DataFrame\n",
        "    df = pd.DataFrame(collected_data)\n",
        "    df.to_csv(\"KMB_data_last_10_years.csv\", index=False, encoding='utf-8')\n",
        "    print(\"Data collection complete. Saved to KMB_data_last_10_years.csv.\")\n",
        "\n",
        "# Run the data collection function\n",
        "collect_kmb_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1d9FVGu3aZF",
        "outputId": "1cb7c660-6d65-41bd-e9ed-6b41bd0faa31"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data collection complete. Saved to KMB_data_last_10_years.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv"
      ],
      "metadata": {
        "id": "v4XNykg47yah"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_issuers(filename='issuers.csv'):\n",
        "    # Read issuers from the CSV file\n",
        "    issuers = []\n",
        "    with open(filename, mode='r', newline='', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)  # Skip the header if there's one\n",
        "        for row in reader:\n",
        "            issuers.append(row[0].strip())\n",
        "    return issuers"
      ],
      "metadata": {
        "id": "0t2w2R-77Wwu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_issuers(filename='issuers.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lhzj3fHw7Zse",
        "outputId": "594e7a38-8bcf-4b9e-d555-516e38670d80"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ADIN',\n",
              " 'ALK',\n",
              " 'ALKB',\n",
              " 'AMEH',\n",
              " 'APTK',\n",
              " 'ATPP',\n",
              " 'AUMK',\n",
              " 'BANA',\n",
              " 'BGOR',\n",
              " 'BIKF',\n",
              " 'BIM',\n",
              " 'BLTU',\n",
              " 'CBNG',\n",
              " 'CDHV',\n",
              " 'CEVI',\n",
              " 'CKB',\n",
              " 'CKBKO',\n",
              " 'DEBA',\n",
              " 'DIMI',\n",
              " 'EDST',\n",
              " 'ELMA',\n",
              " 'ELNC',\n",
              " 'ENER',\n",
              " 'ENSA',\n",
              " 'EUHA',\n",
              " 'EUMK',\n",
              " 'EVRO',\n",
              " 'FAKM',\n",
              " 'FERS',\n",
              " 'FKTL',\n",
              " 'FROT',\n",
              " 'FUBT',\n",
              " 'GALE',\n",
              " 'GDKM',\n",
              " 'GECK',\n",
              " 'GECT',\n",
              " 'GIMS',\n",
              " 'GRDN',\n",
              " 'GRNT',\n",
              " 'GRSN',\n",
              " 'GRZD',\n",
              " 'GTC',\n",
              " 'GTRG',\n",
              " 'IJUG',\n",
              " 'INB',\n",
              " 'INHO',\n",
              " 'INOV',\n",
              " 'INPR',\n",
              " 'INTP',\n",
              " 'JAKO',\n",
              " 'JUSK',\n",
              " 'KARO',\n",
              " 'KDFO',\n",
              " 'KJUBI',\n",
              " 'KKST',\n",
              " 'KLST',\n",
              " 'KMB',\n",
              " 'KMPR',\n",
              " 'KOMU',\n",
              " 'KONF',\n",
              " 'KONZ',\n",
              " 'KORZ',\n",
              " 'KPSS',\n",
              " 'KULT',\n",
              " 'KVAS',\n",
              " 'LAJO',\n",
              " 'LHND',\n",
              " 'LOTO',\n",
              " 'LOZP',\n",
              " 'MAGP',\n",
              " 'MAKP',\n",
              " 'MAKS',\n",
              " 'MB',\n",
              " 'MERM',\n",
              " 'MKSD',\n",
              " 'MLKR',\n",
              " 'MODA',\n",
              " 'MPOL',\n",
              " 'MPT',\n",
              " 'MPTE',\n",
              " 'MTUR',\n",
              " 'MZHE',\n",
              " 'MZPU',\n",
              " 'NEME',\n",
              " 'NOSK',\n",
              " 'OBPP',\n",
              " 'OILK',\n",
              " 'OKTA',\n",
              " 'OMOS',\n",
              " 'OPFO',\n",
              " 'OPTK',\n",
              " 'ORAN',\n",
              " 'OSPO',\n",
              " 'OTEK',\n",
              " 'PELK',\n",
              " 'PGGV',\n",
              " 'PKB',\n",
              " 'POPK',\n",
              " 'PPIV',\n",
              " 'PROD',\n",
              " 'PROT',\n",
              " 'PTRS',\n",
              " 'RADE',\n",
              " 'REPL',\n",
              " 'RIMI',\n",
              " 'RINS',\n",
              " 'RZEK',\n",
              " 'RZIT',\n",
              " 'RZIZ',\n",
              " 'RZLE',\n",
              " 'RZLV',\n",
              " 'RZTK',\n",
              " 'RZUG',\n",
              " 'RZUS',\n",
              " 'SBT',\n",
              " 'SDOM',\n",
              " 'SIL',\n",
              " 'SKON',\n",
              " 'SKP',\n",
              " 'SLAV',\n",
              " 'SNBT',\n",
              " 'SNBTO',\n",
              " 'SOLN',\n",
              " 'SPAZ',\n",
              " 'SPAZP',\n",
              " 'SPOL',\n",
              " 'SSPR',\n",
              " 'STB',\n",
              " 'STBP',\n",
              " 'STIL',\n",
              " 'STOK',\n",
              " 'TAJM',\n",
              " 'TBKO',\n",
              " 'TEAL',\n",
              " 'TEHN',\n",
              " 'TEL',\n",
              " 'TETE',\n",
              " 'TIKV',\n",
              " 'TKPR',\n",
              " 'TKVS',\n",
              " 'TNB',\n",
              " 'TRDB',\n",
              " 'TRPS',\n",
              " 'TRUB',\n",
              " 'TSMP',\n",
              " 'TSZS',\n",
              " 'TTK',\n",
              " 'TTKO',\n",
              " 'UNI',\n",
              " 'USJE',\n",
              " 'VARG',\n",
              " 'VFPM',\n",
              " 'VITA',\n",
              " 'VROS',\n",
              " 'VSC',\n",
              " 'VTKS',\n",
              " 'ZAS',\n",
              " 'ZILU',\n",
              " 'ZILUP',\n",
              " 'ZIMS',\n",
              " 'ZKAR',\n",
              " 'ZPKO',\n",
              " 'ZPOG',\n",
              " 'ZUAS']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_issuers(filename='issuers.csv'):\n",
        "    # Read issuers from the CSV file\n",
        "    issuers = []\n",
        "    with open(filename, mode='r', newline='', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)  # Skip the header if there's one\n",
        "        for row in reader:\n",
        "            issuers.append(row[0].strip())\n",
        "    return issuers\n",
        "\n",
        "def collect_issuer_data(issuer_code):\n",
        "    # Define the date range for data collection\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=365 * 10)  # Start date 10 years ago\n",
        "\n",
        "    # Base URL and headers\n",
        "    base_url = \"https://www.mse.mk/mk/stats/symbolhistory/kmb\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    # List to store data for the issuer\n",
        "    issuer_data = []\n",
        "\n",
        "    while start_date < end_date:\n",
        "        # Set the range to a maximum of 1 year\n",
        "        range_end_date = min(start_date + timedelta(days=365), end_date)\n",
        "        start_date_str = start_date.strftime('%d.%m.%Y')\n",
        "        range_end_date_str = range_end_date.strftime('%d.%m.%Y')\n",
        "\n",
        "        # Prepare payload for the post request\n",
        "        payload = {\n",
        "            'Code': issuer_code,\n",
        "            'FromDate': start_date_str,\n",
        "            'ToDate': range_end_date_str,\n",
        "            'action': 'Прикажи'\n",
        "        }\n",
        "\n",
        "        # Send POST request to retrieve the data\n",
        "        response = requests.post(base_url, headers=headers, data=payload)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse the response HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Locate the results table and rows within it\n",
        "        table = soup.find(\"table\", {\"id\": \"resultsTable\"})\n",
        "        if not table:\n",
        "            print(f\"No data available for {issuer_code} from {start_date_str} to {range_end_date_str}\")\n",
        "            start_date = range_end_date + timedelta(days=1)\n",
        "            continue\n",
        "\n",
        "        # Process each row in the table (skipping the header row)\n",
        "        rows = table.find_all(\"tr\")[1:]\n",
        "        for row in rows:\n",
        "            columns = row.find_all(\"td\")\n",
        "            if len(columns) >= 9:\n",
        "                data = {\n",
        "                    'Issuer': issuer_code,\n",
        "                    'Date': columns[0].text.strip(),\n",
        "                    'Last Transaction Price': columns[1].text.strip(),\n",
        "                    'Max': columns[2].text.strip(),\n",
        "                    'Min': columns[3].text.strip(),\n",
        "                    'Average Price': columns[4].text.strip(),\n",
        "                    '% Change': columns[5].text.strip(),\n",
        "                    'Quantity': columns[6].text.strip(),\n",
        "                    'Trading Volume (Denars)': columns[7].text.strip(),\n",
        "                    'Total Volume (Denars)': columns[8].text.strip()\n",
        "                }\n",
        "                issuer_data.append(data)\n",
        "\n",
        "        # Move to the next one-year range\n",
        "        start_date = range_end_date + timedelta(days=1)\n",
        "\n",
        "    return issuer_data\n",
        "\n",
        "def main():\n",
        "    issuers = load_issuers('issuers.csv')\n",
        "    all_data = []\n",
        "\n",
        "    for issuer_code in issuers:\n",
        "        print(f\"Collecting data for issuer: {issuer_code}\")\n",
        "        issuer_data = collect_issuer_data(issuer_code)\n",
        "        all_data.extend(issuer_data)\n",
        "\n",
        "    # Save all collected data to a CSV file\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.to_csv(\"all_issuers_data_last_10_years.csv\", index=False, encoding='utf-8')\n",
        "    print(\"Data collection complete. Saved to all_issuers_data_last_10_years.csv.\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CkjE2C_73N3",
        "outputId": "bd6e4043-b150-4fce-a5cd-7b68cb327faa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting data for issuer: ADIN\n",
            "Collecting data for issuer: ALK\n",
            "Collecting data for issuer: ALKB\n",
            "Collecting data for issuer: AMEH\n",
            "Collecting data for issuer: APTK\n",
            "Collecting data for issuer: ATPP\n",
            "Collecting data for issuer: AUMK\n",
            "Collecting data for issuer: BANA\n",
            "Collecting data for issuer: BGOR\n",
            "Collecting data for issuer: BIKF\n",
            "No data available for BIKF from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: BIM\n",
            "Collecting data for issuer: BLTU\n",
            "Collecting data for issuer: CBNG\n",
            "No data available for CBNG from 14.11.2016 to 14.11.2017\n",
            "No data available for CBNG from 15.11.2017 to 15.11.2018\n",
            "Collecting data for issuer: CDHV\n",
            "No data available for CDHV from 17.11.2020 to 17.11.2021\n",
            "Collecting data for issuer: CEVI\n",
            "Collecting data for issuer: CKB\n",
            "Collecting data for issuer: CKBKO\n",
            "No data available for CKBKO from 13.11.2014 to 13.11.2015\n",
            "No data available for CKBKO from 14.11.2015 to 13.11.2016\n",
            "No data available for CKBKO from 14.11.2016 to 14.11.2017\n",
            "No data available for CKBKO from 15.11.2017 to 15.11.2018\n",
            "Collecting data for issuer: DEBA\n",
            "Collecting data for issuer: DIMI\n",
            "Collecting data for issuer: EDST\n",
            "No data available for EDST from 16.11.2018 to 16.11.2019\n",
            "No data available for EDST from 17.11.2019 to 16.11.2020\n",
            "No data available for EDST from 17.11.2020 to 17.11.2021\n",
            "Collecting data for issuer: ELMA\n",
            "No data available for ELMA from 17.11.2020 to 17.11.2021\n",
            "No data available for ELMA from 18.11.2021 to 18.11.2022\n",
            "No data available for ELMA from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: ELNC\n",
            "No data available for ELNC from 14.11.2016 to 14.11.2017\n",
            "No data available for ELNC from 15.11.2017 to 15.11.2018\n",
            "No data available for ELNC from 16.11.2018 to 16.11.2019\n",
            "No data available for ELNC from 17.11.2019 to 16.11.2020\n",
            "No data available for ELNC from 18.11.2021 to 18.11.2022\n",
            "No data available for ELNC from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: ENER\n",
            "No data available for ENER from 17.11.2019 to 16.11.2020\n",
            "No data available for ENER from 17.11.2020 to 17.11.2021\n",
            "Collecting data for issuer: ENSA\n",
            "No data available for ENSA from 13.11.2014 to 13.11.2015\n",
            "No data available for ENSA from 14.11.2015 to 13.11.2016\n",
            "No data available for ENSA from 14.11.2016 to 14.11.2017\n",
            "No data available for ENSA from 15.11.2017 to 15.11.2018\n",
            "No data available for ENSA from 16.11.2018 to 16.11.2019\n",
            "No data available for ENSA from 17.11.2019 to 16.11.2020\n",
            "No data available for ENSA from 17.11.2020 to 17.11.2021\n",
            "No data available for ENSA from 18.11.2021 to 18.11.2022\n",
            "No data available for ENSA from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: EUHA\n",
            "Collecting data for issuer: EUMK\n",
            "No data available for EUMK from 17.11.2019 to 16.11.2020\n",
            "No data available for EUMK from 17.11.2020 to 17.11.2021\n",
            "Collecting data for issuer: EVRO\n",
            "Collecting data for issuer: FAKM\n",
            "Collecting data for issuer: FERS\n",
            "Collecting data for issuer: FKTL\n",
            "No data available for FKTL from 13.11.2014 to 13.11.2015\n",
            "No data available for FKTL from 14.11.2015 to 13.11.2016\n",
            "No data available for FKTL from 14.11.2016 to 14.11.2017\n",
            "No data available for FKTL from 15.11.2017 to 15.11.2018\n",
            "No data available for FKTL from 16.11.2018 to 16.11.2019\n",
            "No data available for FKTL from 17.11.2019 to 16.11.2020\n",
            "No data available for FKTL from 17.11.2020 to 17.11.2021\n",
            "No data available for FKTL from 18.11.2021 to 18.11.2022\n",
            "Collecting data for issuer: FROT\n",
            "No data available for FROT from 14.11.2016 to 14.11.2017\n",
            "Collecting data for issuer: FUBT\n",
            "Collecting data for issuer: GALE\n",
            "Collecting data for issuer: GDKM\n",
            "No data available for GDKM from 14.11.2016 to 14.11.2017\n",
            "No data available for GDKM from 17.11.2019 to 16.11.2020\n",
            "No data available for GDKM from 17.11.2020 to 17.11.2021\n",
            "Collecting data for issuer: GECK\n",
            "Collecting data for issuer: GECT\n",
            "Collecting data for issuer: GIMS\n",
            "Collecting data for issuer: GRDN\n",
            "No data available for GRDN from 14.11.2016 to 14.11.2017\n",
            "Collecting data for issuer: GRNT\n",
            "Collecting data for issuer: GRSN\n",
            "No data available for GRSN from 17.11.2020 to 17.11.2021\n",
            "No data available for GRSN from 18.11.2021 to 18.11.2022\n",
            "No data available for GRSN from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: GRZD\n",
            "Collecting data for issuer: GTC\n",
            "Collecting data for issuer: GTRG\n",
            "Collecting data for issuer: IJUG\n",
            "No data available for IJUG from 17.11.2019 to 16.11.2020\n",
            "Collecting data for issuer: INB\n",
            "Collecting data for issuer: INHO\n",
            "Collecting data for issuer: INOV\n",
            "No data available for INOV from 13.11.2014 to 13.11.2015\n",
            "No data available for INOV from 14.11.2015 to 13.11.2016\n",
            "No data available for INOV from 14.11.2016 to 14.11.2017\n",
            "No data available for INOV from 15.11.2017 to 15.11.2018\n",
            "No data available for INOV from 16.11.2018 to 16.11.2019\n",
            "No data available for INOV from 17.11.2019 to 16.11.2020\n",
            "No data available for INOV from 17.11.2020 to 17.11.2021\n",
            "Collecting data for issuer: INPR\n",
            "Collecting data for issuer: INTP\n",
            "Collecting data for issuer: JAKO\n",
            "No data available for JAKO from 14.11.2016 to 14.11.2017\n",
            "No data available for JAKO from 15.11.2017 to 15.11.2018\n",
            "No data available for JAKO from 16.11.2018 to 16.11.2019\n",
            "No data available for JAKO from 17.11.2019 to 16.11.2020\n",
            "No data available for JAKO from 17.11.2020 to 17.11.2021\n",
            "No data available for JAKO from 18.11.2021 to 18.11.2022\n",
            "No data available for JAKO from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: JUSK\n",
            "No data available for JUSK from 15.11.2017 to 15.11.2018\n",
            "No data available for JUSK from 16.11.2018 to 16.11.2019\n",
            "No data available for JUSK from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: KARO\n",
            "Collecting data for issuer: KDFO\n",
            "No data available for KDFO from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: KJUBI\n",
            "Collecting data for issuer: KKST\n",
            "No data available for KKST from 13.11.2014 to 13.11.2015\n",
            "No data available for KKST from 14.11.2015 to 13.11.2016\n",
            "No data available for KKST from 14.11.2016 to 14.11.2017\n",
            "No data available for KKST from 15.11.2017 to 15.11.2018\n",
            "No data available for KKST from 16.11.2018 to 16.11.2019\n",
            "No data available for KKST from 17.11.2019 to 16.11.2020\n",
            "No data available for KKST from 17.11.2020 to 17.11.2021\n",
            "No data available for KKST from 18.11.2021 to 18.11.2022\n",
            "Collecting data for issuer: KLST\n",
            "Collecting data for issuer: KMB\n",
            "Collecting data for issuer: KMPR\n",
            "No data available for KMPR from 17.11.2020 to 17.11.2021\n",
            "Collecting data for issuer: KOMU\n",
            "Collecting data for issuer: KONF\n",
            "Collecting data for issuer: KONZ\n",
            "Collecting data for issuer: KORZ\n",
            "No data available for KORZ from 15.11.2017 to 15.11.2018\n",
            "No data available for KORZ from 16.11.2018 to 16.11.2019\n",
            "No data available for KORZ from 17.11.2019 to 16.11.2020\n",
            "No data available for KORZ from 17.11.2020 to 17.11.2021\n",
            "No data available for KORZ from 18.11.2021 to 18.11.2022\n",
            "No data available for KORZ from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: KPSS\n",
            "Collecting data for issuer: KULT\n",
            "No data available for KULT from 14.11.2016 to 14.11.2017\n",
            "No data available for KULT from 15.11.2017 to 15.11.2018\n",
            "No data available for KULT from 16.11.2018 to 16.11.2019\n",
            "No data available for KULT from 17.11.2019 to 16.11.2020\n",
            "No data available for KULT from 17.11.2020 to 17.11.2021\n",
            "No data available for KULT from 18.11.2021 to 18.11.2022\n",
            "No data available for KULT from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: KVAS\n",
            "Collecting data for issuer: LAJO\n",
            "No data available for LAJO from 14.11.2016 to 14.11.2017\n",
            "No data available for LAJO from 15.11.2017 to 15.11.2018\n",
            "No data available for LAJO from 16.11.2018 to 16.11.2019\n",
            "No data available for LAJO from 17.11.2019 to 16.11.2020\n",
            "No data available for LAJO from 17.11.2020 to 17.11.2021\n",
            "No data available for LAJO from 18.11.2021 to 18.11.2022\n",
            "No data available for LAJO from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: LHND\n",
            "No data available for LHND from 14.11.2016 to 14.11.2017\n",
            "No data available for LHND from 15.11.2017 to 15.11.2018\n",
            "No data available for LHND from 16.11.2018 to 16.11.2019\n",
            "Collecting data for issuer: LOTO\n",
            "Collecting data for issuer: LOZP\n",
            "Collecting data for issuer: MAGP\n",
            "No data available for MAGP from 13.11.2014 to 13.11.2015\n",
            "No data available for MAGP from 14.11.2015 to 13.11.2016\n",
            "No data available for MAGP from 14.11.2016 to 14.11.2017\n",
            "No data available for MAGP from 17.11.2020 to 17.11.2021\n",
            "No data available for MAGP from 18.11.2021 to 18.11.2022\n",
            "No data available for MAGP from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: MAKP\n",
            "Collecting data for issuer: MAKS\n",
            "Collecting data for issuer: MB\n",
            "Collecting data for issuer: MERM\n",
            "Collecting data for issuer: MKSD\n",
            "Collecting data for issuer: MLKR\n",
            "No data available for MLKR from 14.11.2016 to 14.11.2017\n",
            "No data available for MLKR from 15.11.2017 to 15.11.2018\n",
            "No data available for MLKR from 16.11.2018 to 16.11.2019\n",
            "No data available for MLKR from 17.11.2019 to 16.11.2020\n",
            "No data available for MLKR from 17.11.2020 to 17.11.2021\n",
            "No data available for MLKR from 18.11.2021 to 18.11.2022\n",
            "No data available for MLKR from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: MODA\n",
            "Collecting data for issuer: MPOL\n",
            "Collecting data for issuer: MPT\n",
            "Collecting data for issuer: MPTE\n",
            "No data available for MPTE from 14.11.2016 to 14.11.2017\n",
            "No data available for MPTE from 15.11.2017 to 15.11.2018\n",
            "No data available for MPTE from 16.11.2018 to 16.11.2019\n",
            "No data available for MPTE from 17.11.2019 to 16.11.2020\n",
            "No data available for MPTE from 17.11.2020 to 17.11.2021\n",
            "No data available for MPTE from 18.11.2021 to 18.11.2022\n",
            "No data available for MPTE from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: MTUR\n",
            "Collecting data for issuer: MZHE\n",
            "Collecting data for issuer: MZPU\n",
            "Collecting data for issuer: NEME\n",
            "Collecting data for issuer: NOSK\n",
            "Collecting data for issuer: OBPP\n",
            "No data available for OBPP from 14.11.2016 to 14.11.2017\n",
            "No data available for OBPP from 15.11.2017 to 15.11.2018\n",
            "No data available for OBPP from 16.11.2018 to 16.11.2019\n",
            "No data available for OBPP from 17.11.2019 to 16.11.2020\n",
            "No data available for OBPP from 17.11.2020 to 17.11.2021\n",
            "No data available for OBPP from 18.11.2021 to 18.11.2022\n",
            "No data available for OBPP from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: OILK\n",
            "Collecting data for issuer: OKTA\n",
            "Collecting data for issuer: OMOS\n",
            "No data available for OMOS from 13.11.2014 to 13.11.2015\n",
            "No data available for OMOS from 14.11.2015 to 13.11.2016\n",
            "No data available for OMOS from 14.11.2016 to 14.11.2017\n",
            "No data available for OMOS from 15.11.2017 to 15.11.2018\n",
            "No data available for OMOS from 16.11.2018 to 16.11.2019\n",
            "No data available for OMOS from 17.11.2019 to 16.11.2020\n",
            "Collecting data for issuer: OPFO\n",
            "No data available for OPFO from 13.11.2014 to 13.11.2015\n",
            "No data available for OPFO from 14.11.2015 to 13.11.2016\n",
            "No data available for OPFO from 14.11.2016 to 14.11.2017\n",
            "No data available for OPFO from 15.11.2017 to 15.11.2018\n",
            "No data available for OPFO from 16.11.2018 to 16.11.2019\n",
            "No data available for OPFO from 17.11.2019 to 16.11.2020\n",
            "Collecting data for issuer: OPTK\n",
            "Collecting data for issuer: ORAN\n",
            "Collecting data for issuer: OSPO\n",
            "No data available for OSPO from 14.11.2016 to 14.11.2017\n",
            "No data available for OSPO from 15.11.2017 to 15.11.2018\n",
            "No data available for OSPO from 16.11.2018 to 16.11.2019\n",
            "No data available for OSPO from 17.11.2019 to 16.11.2020\n",
            "No data available for OSPO from 17.11.2020 to 17.11.2021\n",
            "No data available for OSPO from 18.11.2021 to 18.11.2022\n",
            "No data available for OSPO from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: OTEK\n",
            "Collecting data for issuer: PELK\n",
            "No data available for PELK from 16.11.2018 to 16.11.2019\n",
            "Collecting data for issuer: PGGV\n",
            "No data available for PGGV from 17.11.2020 to 17.11.2021\n",
            "No data available for PGGV from 18.11.2021 to 18.11.2022\n",
            "No data available for PGGV from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: PKB\n",
            "Collecting data for issuer: POPK\n",
            "Collecting data for issuer: PPIV\n",
            "Collecting data for issuer: PROD\n",
            "Collecting data for issuer: PROT\n",
            "No data available for PROT from 13.11.2014 to 13.11.2015\n",
            "No data available for PROT from 14.11.2015 to 13.11.2016\n",
            "No data available for PROT from 14.11.2016 to 14.11.2017\n",
            "No data available for PROT from 15.11.2017 to 15.11.2018\n",
            "No data available for PROT from 16.11.2018 to 16.11.2019\n",
            "No data available for PROT from 17.11.2019 to 16.11.2020\n",
            "No data available for PROT from 17.11.2020 to 17.11.2021\n",
            "No data available for PROT from 18.11.2021 to 18.11.2022\n",
            "No data available for PROT from 19.11.2022 to 19.11.2023\n",
            "Collecting data for issuer: PTRS\n",
            "No data available for PTRS from 14.11.2016 to 14.11.2017\n",
            "No data available for PTRS from 15.11.2017 to 15.11.2018\n",
            "No data available for PTRS from 18.11.2021 to 18.11.2022\n",
            "Collecting data for issuer: RADE\n",
            "Collecting data for issuer: REPL\n",
            "Collecting data for issuer: RIMI\n",
            "Collecting data for issuer: RINS\n",
            "No data available for RINS from 16.11.2018 to 16.11.2019\n",
            "No data available for RINS from 17.11.2019 to 16.11.2020\n",
            "No data available for RINS from 17.11.2020 to 17.11.2021\n",
            "No data available for RINS from 18.11.2021 to 18.11.2022\n",
            "Collecting data for issuer: RZEK\n",
            "Collecting data for issuer: RZIT\n",
            "Collecting data for issuer: RZIZ\n",
            "Collecting data for issuer: RZLE\n",
            "Collecting data for issuer: RZLV\n",
            "Collecting data for issuer: RZTK\n",
            "Collecting data for issuer: RZUG\n",
            "Collecting data for issuer: RZUS\n",
            "Collecting data for issuer: SBT\n",
            "Collecting data for issuer: SDOM\n",
            "Collecting data for issuer: SIL\n",
            "Collecting data for issuer: SKON\n",
            "No data available for SKON from 17.11.2019 to 16.11.2020\n",
            "No data available for SKON from 17.11.2020 to 17.11.2021\n",
            "Collecting data for issuer: SKP\n",
            "Collecting data for issuer: SLAV\n",
            "Collecting data for issuer: SNBT\n",
            "No data available for SNBT from 13.11.2014 to 13.11.2015\n",
            "No data available for SNBT from 14.11.2015 to 13.11.2016\n",
            "No data available for SNBT from 14.11.2016 to 14.11.2017\n",
            "No data available for SNBT from 15.11.2017 to 15.11.2018\n",
            "No data available for SNBT from 17.11.2020 to 17.11.2021\n",
            "Collecting data for issuer: SNBTO\n",
            "No data available for SNBTO from 13.11.2014 to 13.11.2015\n",
            "No data available for SNBTO from 14.11.2015 to 13.11.2016\n",
            "No data available for SNBTO from 14.11.2016 to 14.11.2017\n",
            "No data available for SNBTO from 15.11.2017 to 15.11.2018\n",
            "No data available for SNBTO from 16.11.2018 to 16.11.2019\n",
            "No data available for SNBTO from 17.11.2019 to 16.11.2020\n",
            "No data available for SNBTO from 17.11.2020 to 17.11.2021\n",
            "Collecting data for issuer: SOLN\n",
            "Collecting data for issuer: SPAZ\n",
            "Collecting data for issuer: SPAZP\n",
            "Collecting data for issuer: SPOL\n",
            "Collecting data for issuer: SSPR\n",
            "Collecting data for issuer: STB\n",
            "Collecting data for issuer: STBP\n",
            "Collecting data for issuer: STIL\n",
            "Collecting data for issuer: STOK\n",
            "Collecting data for issuer: TAJM\n",
            "Collecting data for issuer: TBKO\n",
            "No data available for TBKO from 17.11.2019 to 16.11.2020\n",
            "Collecting data for issuer: TEAL\n",
            "Collecting data for issuer: TEHN\n",
            "Collecting data for issuer: TEL\n",
            "Collecting data for issuer: TETE\n",
            "Collecting data for issuer: TIKV\n",
            "Collecting data for issuer: TKPR\n",
            "Collecting data for issuer: TKVS\n",
            "Collecting data for issuer: TNB\n",
            "Collecting data for issuer: TRDB\n",
            "Collecting data for issuer: TRPS\n",
            "Collecting data for issuer: TRUB\n",
            "No data available for TRUB from 16.11.2018 to 16.11.2019\n",
            "No data available for TRUB from 18.11.2021 to 18.11.2022\n",
            "Collecting data for issuer: TSMP\n",
            "Collecting data for issuer: TSZS\n",
            "No data available for TSZS from 14.11.2016 to 14.11.2017\n",
            "No data available for TSZS from 15.11.2017 to 15.11.2018\n",
            "No data available for TSZS from 16.11.2018 to 16.11.2019\n",
            "No data available for TSZS from 17.11.2019 to 16.11.2020\n",
            "No data available for TSZS from 17.11.2020 to 17.11.2021\n",
            "No data available for TSZS from 18.11.2021 to 18.11.2022\n",
            "Collecting data for issuer: TTK\n",
            "Collecting data for issuer: TTKO\n",
            "No data available for TTKO from 13.11.2014 to 13.11.2015\n",
            "No data available for TTKO from 14.11.2015 to 13.11.2016\n",
            "No data available for TTKO from 14.11.2016 to 14.11.2017\n",
            "No data available for TTKO from 15.11.2017 to 15.11.2018\n",
            "Collecting data for issuer: UNI\n",
            "Collecting data for issuer: USJE\n",
            "Collecting data for issuer: VARG\n",
            "No data available for VARG from 14.11.2016 to 14.11.2017\n",
            "No data available for VARG from 15.11.2017 to 15.11.2018\n",
            "Collecting data for issuer: VFPM\n",
            "No data available for VFPM from 13.11.2014 to 13.11.2015\n",
            "No data available for VFPM from 14.11.2015 to 13.11.2016\n",
            "No data available for VFPM from 14.11.2016 to 14.11.2017\n",
            "No data available for VFPM from 15.11.2017 to 15.11.2018\n",
            "No data available for VFPM from 16.11.2018 to 16.11.2019\n",
            "No data available for VFPM from 17.11.2019 to 16.11.2020\n",
            "No data available for VFPM from 17.11.2020 to 17.11.2021\n",
            "Collecting data for issuer: VITA\n",
            "Collecting data for issuer: VROS\n",
            "Collecting data for issuer: VSC\n",
            "No data available for VSC from 14.11.2016 to 14.11.2017\n",
            "Collecting data for issuer: VTKS\n",
            "Collecting data for issuer: ZAS\n",
            "Collecting data for issuer: ZILU\n",
            "Collecting data for issuer: ZILUP\n",
            "Collecting data for issuer: ZIMS\n",
            "Collecting data for issuer: ZKAR\n",
            "Collecting data for issuer: ZPKO\n",
            "Collecting data for issuer: ZPOG\n",
            "Collecting data for issuer: ZUAS\n",
            "No data available for ZUAS from 17.11.2020 to 17.11.2021\n",
            "Data collection complete. Saved to all_issuers_data_last_10_years.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_issuers():\n",
        "    response = requests.get(BASE_URL, headers=HEADERS)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    dropdown = soup.find(\"select\", {\"id\": \"Code\"})\n",
        "    issuers = []\n",
        "\n",
        "    for option in dropdown.find_all(\"option\"):\n",
        "        issuer_code = option.get(\"value\")\n",
        "        if issuer_code and not re.search(r'\\d', issuer_code):\n",
        "            issuers.append(issuer_code)\n",
        "\n",
        "    with open(\"issuers.csv\", \"w\", newline=\"\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Issuer\"])\n",
        "        for issuer in issuers:\n",
        "            writer.writerow([issuer])\n",
        "\n",
        "    print(\"Issuer codes saved to issuers.csv\")\n",
        "    return issuers"
      ],
      "metadata": {
        "id": "daaH6s9mLljg"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}